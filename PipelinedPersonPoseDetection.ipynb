{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438aa03a",
   "metadata": {},
   "source": [
    "This notebook is an example how to pipeline two models. A video stream from a local camera is processed by the person detection model. The person detection results are then processed by the pose detection model, one person bbox at a time.\n",
    "Combined result is then displayed.\n",
    "OpenCV is required to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d4cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import cv2 # OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b67dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to default model zoo\n",
    "zoo = dg.connect_model_zoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4f8e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "def show(img, capt = \"<image>\"):\n",
    "    # show opencv image\n",
    "    cv2.imshow(capt, img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('x') or key == ord('q'):\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "def crop(img, bbox):\n",
    "    # crop opencv image to given bbox\n",
    "    return img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33012fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "people_det_model = zoo.load_model(\"yolo_v5s_person_det--512x512_quant_n2x_orca_1\")\n",
    "pose_model = zoo.load_model(\"mobilenet_v1_posenet_coco_keypoints--353x481_quant_n2x_orca_1\")\n",
    "\n",
    "# adjust pose model properties\n",
    "pose_model.output_pose_threshold = 0.2 # lower threshold\n",
    "pose_model.overlay_line_width = 1\n",
    "pose_model.overlay_alpha = 1\n",
    "pose_model.overlay_show_probabilities = False\n",
    "pose_model.overlay_show_labels = False\n",
    "\n",
    "# select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "people_det_model.image_backend = 'opencv'\n",
    "pose_model.image_backend = 'opencv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7465a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open video stream from local camera #0\n",
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "# define iterator function, which returns frames from camera \n",
    "def source():\n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        yield frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d93242ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # run person detection model on a camera stream\n",
    "    for people in people_det_model.predict_batch(source()):\n",
    "\n",
    "        # prepare list of bboxes of detected person\n",
    "        person_boxes = [person['bbox'] for person in people.results]\n",
    "        if len(person_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        # prepare list of images cropped around each detected person\n",
    "        person_crops = [ crop(people.image, box) for box in person_boxes ]\n",
    "\n",
    "        # for each detected person detect the pose\n",
    "        all_poses = None # accumulated result\n",
    "        for poses, box in zip(pose_model.predict_batch(person_crops), person_boxes):\n",
    "\n",
    "            for r in poses.results: # convert pose coordinates to back to original image\n",
    "                for p in r['landmarks']:\n",
    "                    p['landmark'][0] += box[0]\n",
    "                    p['landmark'][1] += box[1]\n",
    "\n",
    "            if all_poses is None: # accumulate all detected poses\n",
    "                all_poses = poses\n",
    "                all_poses._input_image = people.image_overlay\n",
    "            else:\n",
    "                all_poses._inference_results += poses.results\n",
    "\n",
    "        show(all_poses.image_overlay, \"Poses\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    pass # ignore KeyboardInterrupt errors\n",
    "finally:\n",
    "    cv2.destroyAllWindows() # close OpenCV windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06b63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.release() # release camera stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ecfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

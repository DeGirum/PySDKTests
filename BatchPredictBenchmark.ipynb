{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08e7226",
   "metadata": {},
   "source": [
    "This notebook measures performance of PySDK batch prediction. It reports average frame processing times: core inference time of AI accelerator and total frame inference time. To exclude any pre-processing overhead, the input frame is resized to model input size and converted to binary array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DeGirum PySDK\n",
    "import degirum as dg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4388e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# connect to ai server model zoo\n",
    "ai_server_address = None # fill in the IP address of AI server. Use localhost if running locally\n",
    "zoo = dg.connect_model_zoo(ai_server_address)\n",
    "\n",
    "# list all AI models available for inference\n",
    "zoo.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2baec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AI model 'ssd_mobilenet_v2' for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "# You can use any other model to do benchmark for\n",
    "model = zoo.load_model(\"mobilenet_v2_ssd_coco--300x300_quant_n2x_orca_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take some image and prepare binary array resized to model input size\n",
    "blob = model._preprocessor.forward(\"./images/TwoCats.jpg\")[0]\n",
    "\n",
    "# uncomment this line to include pre-processing overhead\n",
    "# blob = \"./images/TwoCats.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03954556",
   "metadata": {},
   "outputs": [],
   "source": [
    "nframes = 200 # number of frames to measure time for\n",
    "\n",
    "model.measure_time = True # enable time statistics collection\n",
    "model.reset_time_stats() # reset time statistics\n",
    "\n",
    "# perform AI model batch inference on a long list of frames and measure total duration\n",
    "tstart = time.time_ns()\n",
    "for res in model.predict_batch([blob] * nframes):\n",
    "    pass\n",
    "frame_time_ms = (time.time_ns() - tstart) * 1e-6 / nframes\n",
    "\n",
    "ai_time_ms = model.time_stats()['CoreInferenceDuration_ms'].avg\n",
    "\n",
    "print(f\"Average per-frame times ---------------------------------\")\n",
    "print(f\"Pure AI inference time: {ai_time_ms:.1f} ms\")\n",
    "print(f\"Total inference time:   {frame_time_ms:.1f} ms\")\n",
    "print(f\"Non-pipelined overhead: {(frame_time_ms - ai_time_ms):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c1063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
